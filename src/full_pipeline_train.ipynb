{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa15072d",
   "metadata": {},
   "source": [
    "## 🧠 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56223eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainwdrop_log = pd.read_csv('../dataset/cleanedwdrop_log_encode_sales_data.csv')\n",
    "trainwfill_log = pd.read_csv('../dataset/cleanedwfill_log_encode_sales_data.csv')\n",
    "trainwdrop_iso = pd.read_csv('../dataset/cleanedwdrop_iso_encode_sales_data.csv')\n",
    "trainwfill_iso = pd.read_csv('../dataset/cleanedwfill_iso_encode_sales_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainwdrop_log.drop(columns=['year'], inplace=True)\n",
    "trainwdrop_iso.drop(columns=['year'], inplace=True)\n",
    "\n",
    "trainwfill_log.drop(columns=['year', 'manufacturer', 'model'], inplace=True)\n",
    "trainwfill_iso.drop(columns=['year', 'manufacturer', 'model'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odometer can car_age into int\n",
    "trainwdrop_log['car_age'] = trainwdrop_log['car_age'].astype(int)\n",
    "trainwdrop_iso['car_age'] = trainwdrop_iso['car_age'].astype(int)\n",
    "trainwfill_log['car_age'] = trainwfill_log['car_age'].astype(int)\n",
    "trainwfill_iso['car_age'] = trainwfill_iso['car_age'].astype(int)\n",
    "\n",
    "trainwdrop_log['odometer'] = trainwdrop_log['odometer'].astype(int)\n",
    "trainwdrop_iso['odometer'] = trainwdrop_iso['odometer'].astype(int)\n",
    "trainwfill_log['odometer'] = trainwfill_log['odometer'].astype(int)\n",
    "trainwfill_iso['odometer'] = trainwfill_iso['odometer'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RidgeRegression\": Ridge(alpha=1.0),\n",
    "    \"RandomForest\": RandomForestRegressor(n_jobs=-1, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_jobs=-1, random_state=42, verbosity=0, tree_method='gpu_hist')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9767267",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Drop Log\": trainwdrop_log.copy(),\n",
    "    \"Drop Iso\": trainwdrop_iso.copy(),\n",
    "    \"Fill Log\": trainwfill_log.copy(),\n",
    "    \"Fill Iso\": trainwfill_iso.copy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y, y_pred):\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    pmae = (mae / y.mean()) * 100\n",
    "    return r2, rmse, mae, pmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results_no_hyper_tune = []\n",
    "\n",
    "# Total iterations = number of datasets * number of models\n",
    "total_iters = len(datasets) * len(models)\n",
    "\n",
    "# Create progress bar\n",
    "pbar = tqdm(total=total_iters, desc=\"Evaluating Models\", leave=True)\n",
    "\n",
    "# --- LOOP THROUGH DATASETS & MODELS ---\n",
    "for name, df in datasets.items():\n",
    "    X = df.drop(columns=[\"price\"])\n",
    "    y = df[\"price\"]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "        r2, rmse, mae, pmae = evaluate(y, y_pred)\n",
    "\n",
    "        results_no_hyper_tune.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Model\": model_name,\n",
    "            \"R²\": round(r2, 4),\n",
    "            \"RMSE\": round(rmse, 2),\n",
    "            \"MAE\": round(mae, 2),\n",
    "            \"PMAE (%)\": round(pmae, 2)\n",
    "        })\n",
    "\n",
    "        # Update progress\n",
    "        pbar.update(1)\n",
    "\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b19ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results_no_hyper_tune)\n",
    "\n",
    "# Improved styled DataFrame for better header and width display\n",
    "styled_results = results_df.style.format({\n",
    "    \"R²\": \"{:.4f}\",\n",
    "    \"RMSE\": \"{:,.2f}\",\n",
    "    \"MAE\": \"{:,.2f}\",\n",
    "    \"PMAE (%)\": \"{:.2f}\"\n",
    "}).background_gradient(subset=[\"R²\"], cmap=\"Blues\") \\\n",
    "  .background_gradient(subset=[\"RMSE\", \"MAE\", \"PMAE (%)\"], cmap=\"Reds_r\") \\\n",
    "  .set_table_styles([\n",
    "    {\"selector\": \"thead th\", \"props\": [\n",
    "        (\"background-color\", \"#1976D2\"),\n",
    "        (\"color\", \"white\"),\n",
    "        (\"font-weight\", \"bold\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"white-space\", \"nowrap\"),\n",
    "        (\"padding\", \"10px\")\n",
    "    ]},\n",
    "    {\"selector\": \"tbody td\", \"props\": [\n",
    "        (\"border\", \"1px solid #ddd\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"padding\", \"8px\"),\n",
    "        (\"white-space\", \"nowrap\")\n",
    "    ]},\n",
    "    {\"selector\": \"table\", \"props\": [\n",
    "        (\"width\", \"100%\"),\n",
    "        (\"table-layout\", \"fixed\")\n",
    "    ]}\n",
    "]) \\\n",
    "  .set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "styled_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for result in results_no_hyper_tune:\n",
    "#     dataset_name = result[\"Dataset\"]\n",
    "#     model_name = result[\"Model\"]\n",
    "\n",
    "#     # Retrieve the corresponding dataset and model\n",
    "#     df = datasets[dataset_name]\n",
    "#     model = models[model_name]\n",
    "\n",
    "#     X = df.drop(columns=[\"price\"])\n",
    "#     y = df[\"price\"]\n",
    "\n",
    "#     # Generate predictions using cross_val_predict\n",
    "#     y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "\n",
    "#     # Plot the results\n",
    "#     plt.figure(figsize=(18, 14))\n",
    "#     plt.hexbin(y, y_pred, gridsize=70, mincnt=1, linewidths=0.5, edgecolors='gray', cmap='Blues', bins='log')\n",
    "#     plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Perfect Prediction')\n",
    "#     plt.xlabel(\"Actual Price\")\n",
    "#     plt.ylabel(\"Predicted Price\")\n",
    "#     plt.title(f\"{dataset_name} - {model_name}\")\n",
    "#     plt.colorbar(label=\"Density\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results_no_hyper_tune)\n",
    "\n",
    "# Create a pivot table for the heatmap\n",
    "heatmap_data = results_df.pivot(index=\"Model\", columns=\"Dataset\", values=\"R²\")\n",
    "\n",
    "# Calculate the average R² and sort by it\n",
    "heatmap_data[\"Avg R²\"] = heatmap_data.mean(axis=1)\n",
    "heatmap_data.sort_values(\"Avg R²\", ascending=False, inplace=True)\n",
    "heatmap_data.drop(columns=\"Avg R²\", inplace=True)\n",
    "\n",
    "# Red theme color map\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"Reds\",          # red theme\n",
    "    linewidths=0.5,\n",
    "    linecolor='white',\n",
    "    cbar_kws={\"label\": \"R² Score\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Model Performance by R² (Sorted)\", fontsize=16)\n",
    "plt.xlabel(\"Dataset\", fontsize=12)\n",
    "plt.ylabel(\"Model\", fontsize=12)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "# Prepare data\n",
    "X_Fill = datasets[\"Fill Log\"].drop(columns=[\"price\"])\n",
    "y_Fill = datasets[\"Fill Log\"][\"price\"]\n",
    "\n",
    "X_Drop = datasets[\"Drop Log\"].drop(columns=[\"price\"])\n",
    "y_Drop = datasets[\"Drop Log\"][\"price\"]\n",
    "\n",
    "# Fit model\n",
    "model = clone(models[\"RandomForest\"]).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances and select top 20\n",
    "importances_fill = pd.Series(model.feature_importances_, index=X_Fill.columns).sort_values(ascending=False).head(20)\n",
    "\n",
    "# Convert to DataFrame for seaborn\n",
    "top_features_df = importances_fill.reset_index()\n",
    "top_features_df.columns = ['Feature', 'Importance']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=top_features_df,\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    hue='Feature',\n",
    "    palette='Reds_r',\n",
    "    dodge=False,\n",
    "    legend=False  # hide redundant legend\n",
    ")\n",
    "plt.title(\"Top 20 Features - RandomForest (Fill Log Dataset)\", fontsize=14)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fa66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances and select top 20\n",
    "importances_drop = pd.Series(model.feature_importances_, index=X_Drop.columns).sort_values(ascending=False).head(20)\n",
    "\n",
    "# Convert to DataFrame for seaborn\n",
    "top_features_df = importances_drop.reset_index()\n",
    "top_features_df.columns = ['Feature', 'Importance']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=top_features_df,\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    hue='Feature',\n",
    "    palette='Reds_r',\n",
    "    dodge=False,\n",
    "    legend=False  # hide redundant legend\n",
    ")\n",
    "plt.title(\"Top 20 Features - RandomForest (Drop Log Dataset)\", fontsize=14)\n",
    "plt.xlabel(\"Importance\", fontsize=12)\n",
    "plt.ylabel(\"Feature\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc9bda",
   "metadata": {},
   "source": [
    "note : redundency on car_age and year drop `year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_fill * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_drop * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feature_fill = importances_fill[:9].index\n",
    "select_feature_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d441b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feature_drop = importances_drop[:9].index\n",
    "select_feature_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_fill[select_feature_fill].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_drop[select_feature_drop].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcbd77",
   "metadata": {},
   "source": [
    "## 🛠️Feature Selection & Hypertune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"Drop Log\": datasets[\"Drop Log\"][select_feature_drop.tolist() + ['price']].copy(),\n",
    "    \"Drop Iso\": datasets[\"Drop Iso\"][select_feature_drop.tolist() + ['price']].copy(),\n",
    "    \"Fill Log\": datasets[\"Fill Log\"][select_feature_fill.tolist() + ['price']].copy(),\n",
    "    \"Fill Iso\": datasets[\"Fill Iso\"][select_feature_fill.tolist() + ['price']].copy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37fd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "results_tuned = []\n",
    "best_params = {}\n",
    "\n",
    "# --- Early stopping callback ---\n",
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.best_value = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, study, trial):\n",
    "        if study.best_value < self.best_value:\n",
    "            self.best_value = study.best_value\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        if self.counter >= self.patience:\n",
    "            print(f\"⛔️ Early stopping triggered after {self.patience} trials.\")\n",
    "            study.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a007b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tuning functions ---\n",
    "def tune_ridge(trial, X, y):\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-4, 1e3, log=True)  # Wide range for alpha\n",
    "    model = Ridge(alpha=alpha)\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    return sse \n",
    "\n",
    "def tune_rf(trial, X, y):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 150), # fewer trees\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10), # shallower trees\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5), # less granular\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 3), # fewer samples per leaf\n",
    "    }\n",
    "    model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    return sse \n",
    "\n",
    "def tune_xgb(trial, X, y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 32),\n",
    "\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBRegressor(**params, n_jobs=-1)\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "    sse = np.sum((y - y_pred) ** 2)\n",
    "    return sse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main tuning loop ---\n",
    "def run_tuning(dataset_name, model_name, datasets, n_trials=None, patience=None):\n",
    "    X = datasets[dataset_name].drop(columns=[\"price\"])\n",
    "    y = datasets[dataset_name][\"price\"]\n",
    "\n",
    "    # Check if the model has already been trained on this dataset\n",
    "    existing_result_index = next(\n",
    "        (i for i, result in enumerate(results_tuned) \n",
    "         if result[\"Dataset\"] == dataset_name and result[\"Model\"].startswith(model_name)), \n",
    "        None\n",
    "    )\n",
    "\n",
    "    # Set default trial/patience if not specified\n",
    "    if n_trials is None:\n",
    "        n_trials = 100 if model_name == \"RidgeRegression\" else 25\n",
    "    if patience is None:\n",
    "        patience = 40 if model_name == \"RidgeRegression\" else 10\n",
    "\n",
    "    if existing_result_index is not None:\n",
    "        print(f\"✅ {model_name} on {dataset_name} has already been tuned. Retraining and updating results.\")\n",
    "    else:\n",
    "        print(f\"🔧 Starting {model_name} on {dataset_name} with {n_trials} trials\")\n",
    "    \n",
    "\n",
    "    def objective(trial):\n",
    "        if model_name == \"RidgeRegression\":\n",
    "            return tune_ridge(trial, X, y)\n",
    "        elif model_name == \"RandomForest\":\n",
    "            return tune_rf(trial, X, y)\n",
    "        elif model_name == \"XGBoost\":\n",
    "            return tune_xgb(trial, X, y)\n",
    "\n",
    "    early_stopping = EarlyStoppingCallback(patience=patience)\n",
    "\n",
    "    # Progress bar wrapper\n",
    "    with tqdm(total=n_trials, desc=f\"{dataset_name} | {model_name}\", leave=False) as pbar:\n",
    "        def progress_bar_callback(study, trial):\n",
    "            pbar.update(1)\n",
    "\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=n_trials+1,\n",
    "                       callbacks=[early_stopping, progress_bar_callback])\n",
    "\n",
    "    best_params[(dataset_name, model_name)] = study.best_params\n",
    "\n",
    "    # Retrain and evaluate\n",
    "    if model_name == \"RidgeRegression\":\n",
    "        model = Ridge(**study.best_params)\n",
    "    elif model_name == \"RandomForest\":\n",
    "        model = RandomForestRegressor(**study.best_params, random_state=42, n_jobs=-1)\n",
    "    elif model_name == \"XGBoost\":\n",
    "        model = XGBRegressor(**study.best_params, random_state=42, n_jobs=-1, verbosity=0)\n",
    "\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf, n_jobs=-1)\n",
    "\n",
    "    y_true = np.expm1(y) if dataset_name == \"Log\" else y\n",
    "    y_pred = np.expm1(y_pred) if dataset_name == \"Log\" else y_pred\n",
    "\n",
    "    r2, rmse, mae, pmae = evaluate(y_true, y_pred)\n",
    "\n",
    "    # Update or append the result based on whether it's already in results_tuned\n",
    "    result = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Model\": model_name + \" (Tuned)\",\n",
    "        \"R²\": round(r2, 4),\n",
    "        \"RMSE\": round(rmse, 2),\n",
    "        \"MAE\": round(mae, 2),\n",
    "        \"PMAE (%)\": round(pmae, 2)\n",
    "    }\n",
    "\n",
    "    if existing_result_index is not None:\n",
    "        # Update the existing result in results_tuned\n",
    "        results_tuned[existing_result_index] = result\n",
    "    else:\n",
    "        # Append as a new result if it's not already present\n",
    "        results_tuned.append(result)\n",
    "\n",
    "    print(f\"✅ Finished {model_name} on {dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b0484",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"Drop Log\", \"Drop Iso\"]:\n",
    "    for model in [\"RidgeRegression\", \"RandomForest\", \"XGBoost\"]:\n",
    "        run_tuning(dataset, model, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f098627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"Fill Log\", \"Fill Iso\"]:\n",
    "    for model in [\"RidgeRegression\", \"RandomForest\", \"XGBoost\"]:\n",
    "        run_tuning(dataset, model, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case want to train only 1 model --> run_tuning(dataset, model, datasets) ex. run_tuning(\"Fill Log\", \"XGBoost\", datasets)\n",
    "run_tuning(\"Fill Log\", \"XGBoost\", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results_tuned)\n",
    "\n",
    "# Improved styled DataFrame for better header and width display\n",
    "styled_results = results_df.style.format({\n",
    "    \"R²\": \"{:.4f}\",\n",
    "    \"RMSE\": \"{:,.2f}\",\n",
    "    \"MAE\": \"{:,.2f}\",\n",
    "    \"PMAE (%)\": \"{:.2f}\"\n",
    "}).background_gradient(subset=[\"R²\"], cmap=\"Blues\") \\\n",
    "  .background_gradient(subset=[\"RMSE\", \"MAE\", \"PMAE (%)\"], cmap=\"Reds_r\") \\\n",
    "  .set_table_styles([\n",
    "    {\"selector\": \"thead th\", \"props\": [\n",
    "        (\"background-color\", \"#1976D2\"),\n",
    "        (\"color\", \"white\"),\n",
    "        (\"font-weight\", \"bold\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"white-space\", \"nowrap\"),\n",
    "        (\"padding\", \"10px\")\n",
    "    ]},\n",
    "    {\"selector\": \"tbody td\", \"props\": [\n",
    "        (\"border\", \"1px solid #ddd\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"padding\", \"8px\"),\n",
    "        (\"white-space\", \"nowrap\")\n",
    "    ]},\n",
    "    {\"selector\": \"table\", \"props\": [\n",
    "        (\"width\", \"100%\"),\n",
    "        (\"table-layout\", \"fixed\")\n",
    "    ]}\n",
    "]) \\\n",
    "  .set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "styled_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results_tuned to a DataFrame\n",
    "results_tuned_df = pd.DataFrame(results_tuned)\n",
    "\n",
    "# Check if results_tuned_df is empty\n",
    "if results_tuned_df.empty:\n",
    "    print(\"No tuned results available to create a heatmap.\")\n",
    "else:\n",
    "    # Create a pivot table\n",
    "    heatmap_data = results_tuned_df.pivot(index=\"Model\", columns=\"Dataset\", values=\"R²\")\n",
    "\n",
    "    heatmap_data[\"Avg R²\"] = heatmap_data.mean(axis=1)\n",
    "    heatmap_data.sort_values(\"Avg R²\", ascending=False, inplace=True)\n",
    "    heatmap_data.drop(columns=\"Avg R²\", inplace=True)\n",
    "\n",
    "    # Red theme color map\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"Reds\",          # red theme\n",
    "        linewidths=0.5,\n",
    "        linecolor='white',\n",
    "        cbar_kws={\"label\": \"R² Score\"}\n",
    "    )\n",
    "\n",
    "    plt.title(\"Model Performance by R² (Sorted)\", fontsize=16)\n",
    "    plt.xlabel(\"Dataset\", fontsize=12)\n",
    "    plt.ylabel(\"Model\", fontsize=12)\n",
    "    plt.xticks(fontsize=11)\n",
    "    plt.yticks(fontsize=11)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "\n",
    "df_no_tune = pd.DataFrame(results_no_hyper_tune)\n",
    "df_tuned = pd.DataFrame(results_tuned)\n",
    "\n",
    "df_no_tune.to_csv('../dataset/results_no_hyper_tune.csv', index=False)\n",
    "df_tuned.to_csv('../dataset/result_tuned.csv', index=False)\n",
    "\n",
    "\n",
    "# df_no_tune = pd.read_csv('../dataset/results_no_hyper_tune.csv') #use for called from saved result \n",
    "# df_tuned = pd.read_csv('../dataset/result_tuned.csv')\n",
    "\n",
    "# Clean up model/dataset names\n",
    "df_tuned[\"Model\"] = df_tuned[\"Model\"].str.replace(r\" \\(Tuned\\)\", \"\", regex=True)\n",
    "df_tuned[\"Dataset\"] = df_tuned[\"Dataset\"].str.replace(r\" Selected\", \"\", regex=True)\n",
    "\n",
    "# Merge for comparison\n",
    "comparison = pd.merge(\n",
    "    df_no_tune,\n",
    "    df_tuned,\n",
    "    on=[\"Dataset\", \"Model\"],\n",
    "    suffixes=(\"_NoTune\", \"_Tuned\")\n",
    ")\n",
    "\n",
    "# Compute differences\n",
    "comparison[\"Δ R²\"] = comparison[\"R²_Tuned\"] - comparison[\"R²_NoTune\"]\n",
    "comparison[\"Δ RMSE\"] = comparison[\"RMSE_NoTune\"] - comparison[\"RMSE_Tuned\"]\n",
    "comparison[\"Δ MAE\"] = comparison[\"MAE_NoTune\"] - comparison[\"MAE_Tuned\"]\n",
    "comparison[\"Δ PMAE\"] = comparison[\"PMAE (%)_NoTune\"] - comparison[\"PMAE (%)_Tuned\"]\n",
    "\n",
    "# Round for display\n",
    "comparison = comparison.round(3)\n",
    "\n",
    "# Sort by R² improvement\n",
    "comparison.sort_values(by=\"Δ R²\", ascending=False, inplace=True)\n",
    "comparison.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Style the comparison DataFrame\n",
    "styled_comparison = comparison.style.format({\n",
    "    \"R²_NoTune\": \"{:.4f}\",\n",
    "    \"RMSE_NoTune\": \"{:,.2f}\",\n",
    "    \"MAE_NoTune\": \"{:,.2f}\",\n",
    "    \"PMAE (%)_NoTune\": \"{:.2f}\",\n",
    "    \"R²_Tuned\": \"{:.4f}\",\n",
    "    \"RMSE_Tuned\": \"{:,.2f}\",\n",
    "    \"MAE_Tuned\": \"{:,.2f}\",\n",
    "    \"PMAE (%)_Tuned\": \"{:.2f}\",\n",
    "    \"Δ R²\": \"{:.4f}\",\n",
    "    \"Δ RMSE\": \"{:,.2f}\",\n",
    "    \"Δ MAE\": \"{:,.2f}\",\n",
    "    \"Δ PMAE\": \"{:.2f}\"\n",
    "}).background_gradient(subset=[\"R²_NoTune\", \"R²_Tuned\", \"Δ R²\"], cmap=\"Blues\") \\\n",
    "  .background_gradient(subset=[\"RMSE_NoTune\", \"MAE_NoTune\", \"PMAE (%)_NoTune\", \"RMSE_Tuned\", \"MAE_Tuned\", \"PMAE (%)_Tuned\"], cmap=\"Reds_r\") \\\n",
    "  .background_gradient(subset=[\"Δ RMSE\", \"Δ MAE\", \"Δ PMAE\"], cmap=\"Reds\") \\\n",
    "  .set_table_styles([\n",
    "    {\"selector\": \"thead th\", \"props\": [\n",
    "        (\"background-color\", \"#1976D2\"),\n",
    "        (\"color\", \"white\"),\n",
    "        (\"font-weight\", \"bold\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"white-space\", \"nowrap\"),\n",
    "        (\"padding\", \"10px\")\n",
    "    ]},\n",
    "    {\"selector\": \"tbody td\", \"props\": [\n",
    "        (\"border\", \"1px solid #ddd\"),\n",
    "        (\"text-align\", \"center\"),\n",
    "        (\"padding\", \"8px\"),\n",
    "        (\"white-space\", \"nowrap\")\n",
    "    ]},\n",
    "    {\"selector\": \"table\", \"props\": [\n",
    "        (\"width\", \"100%\"),\n",
    "        (\"table-layout\", \"fixed\")\n",
    "    ]}\n",
    "]) \\\n",
    "  .set_properties(**{\"text-align\": \"center\"})\n",
    "\n",
    "styled_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9462e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"R²\", \"RMSE\", \"MAE\", \"PMAE (%)\"]\n",
    "melted = pd.melt(\n",
    "    comparison,\n",
    "    id_vars=[\"Dataset\", \"Model\"],\n",
    "    value_vars=[f\"{m}_{sfx}\" for m in metrics for sfx in [\"NoTune\", \"Tuned\"]],\n",
    "    var_name=\"Metric_Version\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "# Split 'Metric_Version' into two columns: Metric and Version\n",
    "melted[[\"Metric\", \"Version\"]] = melted[\"Metric_Version\"].str.extract(r\"(.*)_((?:NoTune)|(?:Tuned))\")\n",
    "melted.drop(columns=[\"Metric_Version\"], inplace=True)\n",
    "\n",
    "# Plot\n",
    "g = sns.catplot(\n",
    "    data=melted,\n",
    "    kind=\"bar\",\n",
    "    x=\"Score\",\n",
    "    y=\"Model\",\n",
    "    hue=\"Version\",\n",
    "    col=\"Metric\",\n",
    "    row=\"Dataset\",\n",
    "    palette=\"Set2\",\n",
    "    sharex=False,\n",
    "    height=4,\n",
    "    aspect=1.6\n",
    ")\n",
    "\n",
    "g.fig.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle(\"Model Performance Comparison: Tuned vs. NoTune\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
